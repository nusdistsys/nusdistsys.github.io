<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Welcome to NUS Distributed Systems DYOM! on 6.824 DYOM</title><link>https://nusdistsys.github.io/</link><description>Recent content in Welcome to NUS Distributed Systems DYOM! on 6.824 DYOM</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 21 Jan 2020 22:17:51 +0800</lastBuildDate><atom:link href="https://nusdistsys.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Spark</title><link>https://nusdistsys.github.io/notes/spark/</link><pubDate>Fri, 03 Apr 2020 10:39:06 +0800</pubDate><guid>https://nusdistsys.github.io/notes/spark/</guid><description>Spark Motivation Resilient Distributed Databases is a distributed memory abstraction that programmers use to do in-memory computation in a fault-tolerant manner.
Motivated by iterative algorithms and interactive data-mining tools. (These algorithms require intermediate results). RDDs provide restricted form of shared memory.
How does it solve the problem RDDs provide a coarse-grained transformation. The system logs these transformations instead to build the dataset instead of saving the actual data.
This allows recovered of a partition without using replication.</description></item><item><title>CAP</title><link>https://nusdistsys.github.io/notes/cap/</link><pubDate>Thu, 02 Apr 2020 15:22:02 +0800</pubDate><guid>https://nusdistsys.github.io/notes/cap/</guid><description>Tip: Usually analysed in the context of a master with its replicas. Consider reads and writes sent to the replicas.
Theorem Consistency Every read returns the most recent right, or an error.
Eventual Consistency Low latency but stale data Strong Consistency High latency but latest write Availability Every requests receives a non-error response, without the guarantee that it contains the most recent write.
Partition Tolerance System operates despite aribitrary number of messages droppped by network.</description></item><item><title>FaRM (2015)</title><link>https://nusdistsys.github.io/notes/farm/</link><pubDate>Fri, 20 Mar 2020 22:36:50 +0800</pubDate><guid>https://nusdistsys.github.io/notes/farm/</guid><description>.pdf-relative-link-path { font-size: 80%; color: #444; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .</description></item><item><title>Zookeeper</title><link>https://nusdistsys.github.io/notes/zookeeper/</link><pubDate>Thu, 13 Feb 2020 16:04:06 +0800</pubDate><guid>https://nusdistsys.github.io/notes/zookeeper/</guid><description>Leader Elections Leader is needed to order client requests into a transaction.
The leader elected is the node with the smallest zxid.
Each server starts in the LOOKING state, where it must either elect a new leader or find the existing one. If a leader already exists, other servers inform the new one which server is the leader. At this point, the new server connects to the leader and makes sure that its own state is consistent with the state of the leader.</description></item><item><title>Raft</title><link>https://nusdistsys.github.io/notes/raft/</link><pubDate>Tue, 28 Jan 2020 17:25:09 +0800</pubDate><guid>https://nusdistsys.github.io/notes/raft/</guid><description>Raft Summary A consensus algorithm that is usually used to keep replicated logs consistent. Created to be easier to understand (than Paxos). Replicated logs can be used to implement replicated state machines, e.g. distribtued databases. Since state machines are deterministic, feeding them the same instructions lead to the same output, hence keeping the log consistent is important.
Raft:
can tolerate non-byzantine failures e.g. partitions, fail-stop servers, lost/delayed messages does not depend on timing in general, works as long as a majority of servers are up A raft cluster contains several servers, each server is a Follower, Candidate or Leader.</description></item><item><title>Fault-Tolerant Virtual Machines</title><link>https://nusdistsys.github.io/notes/ftvm/</link><pubDate>Thu, 23 Jan 2020 16:23:37 +0800</pubDate><guid>https://nusdistsys.github.io/notes/ftvm/</guid><description>Why use VMs? Insert some answer here
What does it mean to have difficulties in ensuring deterministic execution of physical server (esp as frequencies increase?) Deterministic Replay implementation Non-deterministic events, non-deterministic operations (virtual interrupts, clock cycle counters respectively) Events - remember which instruction at which they occur at Operations: &amp;ldquo;sufficient information&amp;rdquo; is kept about them - such as? remember what they're undefined for and checking for that condition?</description></item><item><title>RPC</title><link>https://nusdistsys.github.io/notes/rpc/</link><pubDate>Wed, 22 Jan 2020 15:12:27 +0800</pubDate><guid>https://nusdistsys.github.io/notes/rpc/</guid><description>What's an RPC?
It is a model of programming. Under the RPC model, requests are location transparent &amp;ndash; a request to a remote network looks the same as calling a function within the same process.
RPCs are best effort.
Helpful Links
[Reasons to choose RPC over REST API] (https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)
More reasons to prefer rest over RPC</description></item><item><title>GFS</title><link>https://nusdistsys.github.io/notes/gfs/</link><pubDate>Wed, 22 Jan 2020 12:04:06 +0800</pubDate><guid>https://nusdistsys.github.io/notes/gfs/</guid><description>Goals for GFS High probability of component failures - requirement to detect and recover promptly from such failures. Common case: a lot of large files, (small files also supported, but not optimised) Support for large streaming reads as well as small random reads Large appends the common use case (to be optimised for this) Atomicity for concurrent appends, with minimal synchronisation overhead High bandwidth over low latency - what does this mean?</description></item></channel></rss>