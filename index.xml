<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NUS Distributed Systems Summer Reading Group on 6.824 DYOM</title><link>https://nusdistsys.github.io/</link><description>Recent content in NUS Distributed Systems Summer Reading Group on 6.824 DYOM</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 18 Jun 2020 14:17:51 +0800</lastBuildDate><atom:link href="https://nusdistsys.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Insert Notes Here</title><link>https://nusdistsys.github.io/summer/sample/</link><pubDate>Thu, 18 Jun 2020 16:04:06 +0800</pubDate><guid>https://nusdistsys.github.io/summer/sample/</guid><description/></item><item><title>Past NUS DYOM!</title><link>https://nusdistsys.github.io/notes/1920s2_course_info/</link><pubDate>Thu, 18 Jun 2020 10:17:51 +0800</pubDate><guid>https://nusdistsys.github.io/notes/1920s2_course_info/</guid><description>This is a group study course on Distributed Systems based on the 6.824 course material
This website serves as a store for our discussion notes and/or questions that arise.
General Info/Notices Our next session would be held on Tuesday 1-2 p.m., venue TBC.
This is a discussion session with Prof Seth, so please come prepared :)
Contributing Check out the source branch for how you can contribute your notes to this repo :D</description></item><item><title>Tail At Scale</title><link>https://nusdistsys.github.io/notes/tailatscale/</link><pubDate>Thu, 23 Apr 2020 15:59:53 +0800</pubDate><guid>https://nusdistsys.github.io/notes/tailatscale/</guid><description>Tail at Scale Problem Temporary high latency episodes may come to dominate service performance at scale. These things occur at the end of the latency curve and are problematic.
Hence, &amp;ldquo;Latency tail-tolerant&amp;rdquo; because we need to optimize for the &amp;ldquo;tail&amp;rdquo; of this latency.
The techniques focus on allowing high system utilization without wasteful overprovisioning.
Sources of variability Shared resources
Daemons
Global Resource Sharing
Maintenence</description></item><item><title>Atomic</title><link>https://nusdistsys.github.io/notes/atomic/</link><pubDate>Tue, 14 Apr 2020 12:18:54 +0800</pubDate><guid>https://nusdistsys.github.io/notes/atomic/</guid><description>Atomic Goal: define atomicity and describe strategies on how it can be achieved Property required in several different areas of computer system design.
There are two main kinds of atomicity, but different sources of literature might define these things differently.
All-or-nothing is regarded as the default definition of atomicity.
Before-or-after is sometimes regarded as Isolation.
There remain some important gaps in our exploration of atomicity. First, in a layered system, a transaction implemented in one layer may consist of a series of component actions of a lower layer that are themselves atomic.</description></item><item><title>Spark</title><link>https://nusdistsys.github.io/notes/spark/</link><pubDate>Fri, 03 Apr 2020 10:39:06 +0800</pubDate><guid>https://nusdistsys.github.io/notes/spark/</guid><description>Spark Motivation Resilient Distributed Databases is a distributed memory abstraction that programmers use to do in-memory computation in a fault-tolerant manner.
Motivated by iterative algorithms and interactive data-mining tools. (These algorithms require intermediate results). RDDs provide restricted form of shared memory.
How does it solve the problem RDDs provide a coarse-grained transformation. The system logs these transformations instead to build the dataset instead of saving the actual data.
This allows recovered of a partition without using replication.</description></item><item><title>CAP</title><link>https://nusdistsys.github.io/notes/cap/</link><pubDate>Thu, 02 Apr 2020 15:22:02 +0800</pubDate><guid>https://nusdistsys.github.io/notes/cap/</guid><description>Tip: Usually analysed in the context of a master with its replicas. Consider reads and writes sent to the replicas.
Theorem Consistency Every read returns the most recent right, or an error.
Eventual Consistency Low latency but stale data Strong Consistency High latency but latest write Availability Every requests receives a non-error response, without the guarantee that it contains the most recent write.
Partition Tolerance System operates despite aribitrary number of messages droppped by network.</description></item><item><title>FaRM (2015)</title><link>https://nusdistsys.github.io/notes/farm/</link><pubDate>Fri, 20 Mar 2020 22:36:50 +0800</pubDate><guid>https://nusdistsys.github.io/notes/farm/</guid><description>.pdf-relative-link-path { font-size: 80%; color: #444; } .callout { border-radius: 3px; padding: 1rem; } figure { margin: 1.25em 0; page-break-inside: avoid; } figcaption { opacity: 0.5; font-size: 85%; margin-top: 0.5em; } mark { background-color: transparent; } .indented { padding-left: 1.5em; } hr { background: transparent; display: block; width: 100%; height: 1px; visibility: visible; border: none; border-bottom: 1px solid rgba(55, 53, 47, 0.09); } img { max-width: 100%; } @media only print { img { max-height: 100vh; object-fit: contain; } } @page { margin: 1in; } .</description></item><item><title>Zookeeper</title><link>https://nusdistsys.github.io/notes/zookeeper/</link><pubDate>Thu, 13 Feb 2020 16:04:06 +0800</pubDate><guid>https://nusdistsys.github.io/notes/zookeeper/</guid><description>Leader Elections Leader is needed to order client requests into a transaction.
The leader elected is the node with the smallest zxid.
Each server starts in the LOOKING state, where it must either elect a new leader or find the existing one. If a leader already exists, other servers inform the new one which server is the leader. At this point, the new server connects to the leader and makes sure that its own state is consistent with the state of the leader.</description></item><item><title>Raft</title><link>https://nusdistsys.github.io/notes/raft/</link><pubDate>Tue, 28 Jan 2020 17:25:09 +0800</pubDate><guid>https://nusdistsys.github.io/notes/raft/</guid><description>Raft Summary A consensus algorithm that is usually used to keep replicated logs consistent. Created to be easier to understand (than Paxos). Replicated logs can be used to implement replicated state machines, e.g. distribtued databases. Since state machines are deterministic, feeding them the same instructions lead to the same output, hence keeping the log consistent is important.
Raft:
can tolerate non-byzantine failures e.g. partitions, fail-stop servers, lost/delayed messages does not depend on timing in general, works as long as a majority of servers are up A raft cluster contains several servers, each server is a Follower, Candidate or Leader.</description></item><item><title>Fault-Tolerant Virtual Machines</title><link>https://nusdistsys.github.io/notes/ftvm/</link><pubDate>Thu, 23 Jan 2020 16:23:37 +0800</pubDate><guid>https://nusdistsys.github.io/notes/ftvm/</guid><description>Why use VMs? Insert some answer here
What does it mean to have difficulties in ensuring deterministic execution of physical server (esp as frequencies increase?) Deterministic Replay implementation Non-deterministic events, non-deterministic operations (virtual interrupts, clock cycle counters respectively) Events - remember which instruction at which they occur at Operations: &amp;ldquo;sufficient information&amp;rdquo; is kept about them - such as? remember what they're undefined for and checking for that condition?</description></item><item><title>RPC</title><link>https://nusdistsys.github.io/notes/rpc/</link><pubDate>Wed, 22 Jan 2020 15:12:27 +0800</pubDate><guid>https://nusdistsys.github.io/notes/rpc/</guid><description>What's an RPC?
It is a model of programming. Under the RPC model, requests are location transparent &amp;ndash; a request to a remote network looks the same as calling a function within the same process.
RPCs are best effort.
Helpful Links
[Reasons to choose RPC over REST API] (https://github.com/donnemartin/system-design-primer#remote-procedure-call-rpc)
More reasons to prefer rest over RPC</description></item><item><title>GFS</title><link>https://nusdistsys.github.io/notes/gfs/</link><pubDate>Wed, 22 Jan 2020 12:04:06 +0800</pubDate><guid>https://nusdistsys.github.io/notes/gfs/</guid><description>Goals for GFS High probability of component failures - requirement to detect and recover promptly from such failures. Common case: a lot of large files, (small files also supported, but not optimised) Support for large streaming reads as well as small random reads Large appends the common use case (to be optimised for this) Atomicity for concurrent appends, with minimal synchronisation overhead High bandwidth over low latency - what does this mean?</description></item></channel></rss>