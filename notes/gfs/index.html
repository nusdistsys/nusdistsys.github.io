<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>GFS | 6.824 DYOM</title><link rel=canonical href=https://nusdistsys.github.io/notes/gfs/><link rel=stylesheet href=https://nusdistsys.github.io/css/base.css integrity crossorigin=anonymous></head><body><nav><div class="px-2 bg-teal-700 fixed w-full"><ul class="flex h-full list-none m-0"><li class=p-4><a class="inline font-medium text-xl text-white" href=https://nusdistsys.github.io/>6.824 DYOM</a></li><li class="flex hover:bg-teal-800 h-full content-center p-4"><a class="inline font-light text-xl text-white" href=https://nusdistsys.github.io/labs/>Labs</a></li><li class="flex hover:bg-teal-800 h-full content-center p-4"><a class="inline font-light text-xl text-white" href=https://nusdistsys.github.io/notes/>Notes</a></li><li class="flex hover:bg-teal-800 h-full content-center p-4"><a class="inline font-light text-xl text-white" href=https://nusdistsys.github.io/summer/>Summer</a></li></ul></div></div></nav><main><div class="container max-w-3xl mx-auto pt-20"><article><header><span class="block py-8"><h1><a class=font-sans href=https://nusdistsys.github.io/notes/gfs/ rel=bookmark>GFS</a></h1><time datetime=2020-01-22T12:04:06+08:00>22 January, 2020</time></span></header><h2 id=goals-for-gfs>Goals for GFS</h2><ol><li>High probability of component failures - requirement to
detect and recover promptly from such failures.</li><li>Common case: a lot of <em>large</em> files, (small files also
supported, but not optimised)</li><li>Support for large streaming reads as well as small random reads</li><li>Large appends the common use case (to be optimised for this)</li><li>Atomicity for concurrent appends, with minimal synchronisation overhead</li><li>High bandwidth over low latency - what does this mean?<ul><li>You care about getting more data over vs low latency?</li></ul></li></ol><h2 id=architecture>Architecture</h2><p><img src=https://2.bp.blogspot.com/-C7Qcn2akF7E/U0zVjII34hI/AAAAAAAAAQY/7Cvy2OX9m9s/w1200-h630-p-k-no-nu/GFS+architecture.JPG alt="GFS architecture"></p><h2 id=chunk-size>Chunk size</h2><h3 id=balancing-between-big-and-small-chunk-size>Balancing between big and small chunk size</h3><ul><li>Larger chunk size: Pros<ul><li>lesser chunks -> lesser metadata overhead on master server</li><li>With a large working set of file locations, client would still be
more able to comfortably cache all chunk information within local
memory</li></ul></li><li>Cons<ul><li>Space wastage due to internal fragmentation (mitigated with lazy allocation)</li><li>If a small file has little number of chunks, accessing the file
would cause the chunkservers to become hotspots (check paper for
mitigation)</li></ul></li></ul><h2 id=guarantees>Guarantees</h2><p><img src=https://i.imgur.com/MCJQPkq.png alt=definitions></p><h2 id=how-it-works>How it works</h2><ul><li>The paper discusses this pretty well and in-depth.</li></ul><h2 id=other-interesting-propertiesmechanisms>Other interesting properties/mechanisms</h2><h3 id=namespace-management-and-locking>Namespace management and locking</h3><ul><li>No per-directory data structure</li><li>When a new file is created:<ul><li>Read locks acquired on all parent directories</li><li>Write lock is acquired on the new file</li></ul></li><li>Implication:<ul><li>Multiple clients can make new files on same directories</li><li>Read lock ensures that directory is not deleted when new file is
being created</li></ul></li></ul><h3 id=garbage-collection-and-stale-replication-detection>Garbage collection and stale replication detection</h3><ul><li>Version numbers are maintained and reported periodically to master
server by the chunk servers</li><li>Checked again when a lease is about to be given to a chunkserver</li></ul><h3 id=checksums>Checksums</h3><ul><li>Computed on each chunkserver - why?</li><li>&ldquo;Corruption&rdquo; - referring to data being corrupted locally due to
equipment failure, as opposed to transport corruption.</li></ul><h3 id=snapshotting>Snapshotting</h3><ul><li>Goal: minimize interruptions of ongoing data.</li><li>Copy on write technique</li></ul><ol><li>When the master receives a snapshot
request, it first revokes any outstanding leases on the chunks
in the files it is about to snapshot.</li><li>It duplicates the metadata
for the source file or directory tree.</li><li>The master notices that the
reference count for chunk C is greater than one. It defers
replying to the client request and instead picks a new chunk handle C’. It then asks each chunkserver that has a current
replica of C to create a new chunk called C’.
(Local copy decreases overhead)</li></ol><h2 id=questions>Questions:</h2><h3 id=hadoop>Hadoop:</h3><h4 id=what-does-it-run-on-hdfs-vs-gfs>What does it run on? HDFS vs GFS?</h4><ul><li>GFS is an earlier version of HDFS</li></ul><h3 id=how-about-the-checksum-operation-and-io-what-does-this-mean>How about the checksum operation and I/O? what does this mean?</h3><ul><li>What happens if send IO before checksum is completed</li></ul></article></div></main></body></html>