<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Spark | 6.824 DYOM</title><link rel=canonical href=https://nusdistsys.github.io/notes/spark/><link rel=stylesheet href=https://nusdistsys.github.io/css/base.css integrity crossorigin=anonymous></head><body><nav><div class="px-2 bg-teal-700 fixed w-full"><ul class="flex h-full list-none m-0"><li class=p-4><a class="inline font-medium text-xl text-white" href=https://nusdistsys.github.io/>6.824 DYOM</a></li><li class="flex hover:bg-teal-800 h-full content-center p-4"><a class="inline font-light text-xl text-white" href=https://nusdistsys.github.io/labs/>Labs</a></li><li class="flex hover:bg-teal-800 h-full content-center p-4"><a class="inline font-light text-xl text-white" href=https://nusdistsys.github.io/notes/>Notes</a></li><li class="flex hover:bg-teal-800 h-full content-center p-4"><a class="inline font-light text-xl text-white" href=https://nusdistsys.github.io/summer/>Summer</a></li></ul></div></div></nav><main><div class="container max-w-3xl mx-auto pt-20"><article><header><span class="block py-8"><h1><a class=font-sans href=https://nusdistsys.github.io/notes/spark/ rel=bookmark>Spark</a></h1><time datetime=2020-04-03T10:39:06+08:00>3 April, 2020</time></span></header><h1 id=spark>Spark</h1><h2 id=motivation>Motivation</h2><p><em>Resilient Distributed Databases</em> is a distributed memory abstraction that programmers use to do in-memory computation in a fault-tolerant manner.</p><p>Motivated by <em>iterative</em> algorithms and <em>interactive data-mining tools</em>. (These algorithms require <strong>intermediate</strong> results). RDDs provide restricted form of shared memory.</p><h2 id=how-does-it-solve-the-problem>How does it solve the problem</h2><p>RDDs provide a <em>coarse-grained</em> transformation. The system logs these transformations instead to build the dataset instead of saving the actual data.</p><p>This allows recovered of a partition without using <em>replication</em>.</p><h2 id=more-on-fault-tolerance-guarantees>More on fault tolerance guarantees</h2><p>The main challenge in designing RDDs is defining a programming interface that can provide fault tolerance efficiently. Existing abstractions for in-memory storage on clusters, such as distributed shared memory [24], keyvalue stores [25], databases, and Piccolo [27], offer an interface based on fine-grained updates to mutable state (e.g., cells in a table). With this interface, the only ways to provide fault tolerance are to replicate the data across machines or to log updates across machines. Both approaches are expensive for data-intensive workloads, as they require copying large amounts of data over the cluster network, whose bandwidth is far lower than that of RAM, and they incur substantial storage overhead.</p><p>In contrast to these systems, RDDs provide an interface based on coarse-grained transformations (e.g., map, filter and join) that apply the same operation to many data items. This allows them to efficiently provide fault tolerance by logging the transformations used to build a dataset (its lineage) rather than the actual data.1 If a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to recompute just that partition. Thus, lost data can be recovered, often quite quickly, without requiring costly replication.</p><p>Although an interface based on coarse-grained transformations may at first seem limited, RDDs are a good fit for many parallel applications, because these applications naturally apply the same operation to multiple data items.</p><h2 id=programming-interface>Programming Interface</h2><h3 id=rdd-operations>RDD Operations</h3><ul><li><p><em>Transformations</em>: Lazy operations that define a new RDD</p></li><li><p><em>Actions</em>: Launch a computation to return value to program/write to external storage.</p></li></ul><p>There exists both narrow dependencies and wide dependencies.</p><ul><li><p>Narrow dependecies: allow pipelined execution on one cluster node</p></li><li><p>Wide dependencies: Wait for parent partitions to be completed before shuffling across everything in a map-reduce like manner</p></li></ul><h2 id=implementation>Implementation</h2><h3 id=job-scheduling>Job scheduling</h3><ul><li><p>Lineage graph forms a DAG, DAG contains pipelined transformations with narrow dependencies.</p></li><li><p>Tasks are assigned based on data locality</p></li><li><p>For shuffle dependencies, we materialize intermediate records.</p></li><li><p>If task processes a partition that is available in memory on a node, we send it to that node.</p></li></ul></article></div></main></body></html>